pipeline {
    agent any
    
    environment {
        APP_DIR = '/app'
        EXTERNAL_IP = sh(script: 'curl -s ifconfig.me', returnStdout: true).trim()
    }
    
    stages {
        stage('Cloning') {
            steps {
                echo "Cloning your app"
                git url: "https://github.com/ifeelpankaj/prod-root.git",
                branch: "master"
            }
        }
        stage('System Prerequisites') {
            steps {
                sh '''
                    echo "Setting up system requirements for ELK Stack..."
                    
                    # Set vm.max_map_count for Elasticsearch (required)
                    sudo sysctl -w vm.max_map_count=262144
                    
                    # Make it persistent
                    if ! grep -q "vm.max_map_count=262144" /etc/sysctl.conf; then
                        echo "vm.max_map_count=262144" |sudo tee -a /etc/sysctl.conf
                    fi
                    
                    # Check system resources
                    echo "System Resources:"
                    free -h
                    df -h .
                '''
            }
        }
        stage('Verify ELK Configuration') {
            steps {
                sh '''
                    echo "Verifying ELK Stack configuration files..."

                    # Check if configuration files exist
                    if [ ! -f "filebeat/filebeat.yml" ]; then
                        echo "ERROR: filebeat/filebeat.yml not found!"
                        exit 1
                    fi
                    
                    if [ ! -f "logstash/config/logstash.yml" ]; then
                        echo "ERROR: logstash/config/logstash.yml not found!"
                        exit 1
                    fi
                    
                    if [ ! -f "logstash/pipeline/logstash.conf" ]; then
                        echo "ERROR: logstash/pipeline/logstash.conf not found!"
                        exit 1
                    fi
                    
                    # Set proper permissions
                    chmod 644 filebeat/filebeat.yml
                    chmod 644 logstash/config/logstash.yml
                    chmod 644 logstash/pipeline/logstash.conf
                    
                    echo "All ELK configuration files found and permissions set!"
                    echo "Configuration files:"
                    ls -la filebeat/
                    ls -la logstash/config/
                    ls -la logstash/pipeline/
                '''
            }
        }
        stage('Prepare MongoDB Keyfile') {
            steps {
                sh '''
                    # Create directory
                    mkdir -p mongo-keyfile
                    
                    # Generate keyfile using Docker (avoids permission issues)
                    docker run --rm -v $(pwd)/mongo-keyfile:/keyfile alpine:latest sh -c "
                        apk add --no-cache openssl &&
                        openssl rand -base64 756 > /keyfile/mongodb-keyfile &&
                        chmod 400 /keyfile/mongodb-keyfile &&
                        chown 999:999 /keyfile/mongodb-keyfile
                    "
                '''
            }
        }
        stage('Setup Client Environment') {
            steps {
                dir('prod-client') {
                    sh '''
                        #!/bin/bash
                        touch .env
                        cp ${APP_DIR}/.env .env
                        sed -i "s|VITE_SERVER=http://localhost:4000|VITE_SERVER=http://${EXTERNAL_IP}:4000|g" .env
                    '''
                }
            }
        }
        
        stage('Setup Admin Environment') {
            steps {
                dir('prod-admin') {
                    sh '''
                        #!/bin/bash
                        touch .env
                        cp ${APP_DIR}/.env .env
                        sed -i "s|VITE_SERVER=http://localhost:4000|VITE_SERVER=http://${EXTERNAL_IP}:4000|g" .env
                    '''
                }
            }
        }
        stage('Setup Server Environment') {
            steps {
                dir('prod-server') {
                    sh '''
                        #!/bin/bash
                        touch .env.development .env.production
                        cp ${APP_DIR}/.env.development .env.development
                        cp ${APP_DIR}/.env.production .env.production
                        
                        sed -i '/^ALLOWED_ORIGINS=/s|localhost|'${EXTERNAL_IP}'|g' .env.production
                        sed -i '/^ALLOWED_ORIGINS=/s|localhost|'${EXTERNAL_IP}'|g' .env.development
                    '''
                }
            }
        }
        
        stage('Deploy with Docker') {
            steps {
                sh '''
                    echo "Deploying application with ELK Stack..."
                    
                    # Use Docker to create logs directory with proper permissions
                    echo "Setting up logs directory using Docker..."
                    
                    # Use alpine container to create logs directory
                    docker run --rm \
                        -v $(pwd)/prod-server:/app \
                        -w /app \
                        alpine:latest \
                        sh -c "
                            mkdir -p logs && 
                            touch logs/production.log logs/development.log && 
                            chmod -R 777 logs &&
                            ls -la logs/
                        "
                    
                    echo "Logs directory created successfully using Docker"
                    # Clean up existing containers and volumes
                    docker compose down --remove-orphans
                    docker builder prune -f
                    
                    # Build without cache
                    docker compose build --no-cache
                    
                    # Start MongoDB first
                    echo "Starting MongoDB..."
                    docker compose up -d mongodb
                    
                    # Wait for MongoDB to be healthy
                    echo "Waiting for MongoDB to be healthy..."
                    timeout 120 sh -c 'until docker compose ps mongodb | grep -q "healthy"; do sleep 5; done'
                    
                    # Initialize replica set
                    echo "Initializing MongoDB replica set..."
                    docker compose up mongodb-init --abort-on-container-exit
                    
                    # Start Elasticsearch first (it takes longest to start)
                    echo "Starting Elasticsearch..."
                    docker compose up -d elasticsearch
                    
                    # Wait for Elasticsearch to be healthy
                    echo "Waiting for Elasticsearch to be healthy (this may take 2-3 minutes)..."
                    timeout 300 sh -c 'until docker compose ps elasticsearch | grep -q "healthy"; do sleep 10; done'
                    
                    # Start remaining ELK services
                    echo "Starting Logstash and Kibana..."
                    docker compose up -d logstash kibana filebeat
                    
                    # Start application services
                    echo "Starting application services..."
                    docker compose up -d backend frontend admin
                    
                    # Wait a moment for services to stabilize
                    sleep 30
                    
                    # Verify all services are running
                    echo "Verifying all services..."
                    docker compose ps
                    
                    # Check ELK Stack health
                    echo "Checking ELK Stack health..."
                    timeout 60 sh -c 'until curl -s http://localhost:9200/_cluster/health | grep -q "yellow\\|green"; do sleep 5; done' || echo "Elasticsearch not fully ready yet"
                    timeout 60 sh -c 'until curl -s http://localhost:5601/api/status | grep -q "available"; do sleep 5; done' || echo "Kibana not fully ready yet"
                '''
            }
        }
        stage('Post-Deployment Health Check') {
            steps {
                sh '''
                    echo "Performing post-deployment health checks..."
                    
                    # Check application services
                    echo "Application Health:"
                    curl -f http://localhost:4000/api/v1/system/health || echo "Backend health check failed"
                    curl -f http://localhost:4173 || echo "Frontend health check failed"
                    curl -f http://localhost:4174 || echo "Admin health check failed"
                    
                    # Check ELK Stack
                    echo "ELK Stack Health:"
                    curl -s http://localhost:9200/_cluster/health | jq '.' || echo "Elasticsearch not responding"
                    curl -s http://localhost:9200/_cat/indices?v || echo "No indices created yet"
                    curl -s http://localhost:5601/api/status || echo "Kibana not responding"
                    
                    # Check if logs are being processed
                    echo "Log Processing Status:"
                    docker compose logs --tail=20 filebeat
                    docker compose logs --tail=20 logstash
                    
                    echo "Deployment complete!"
                    echo "Access URLs:"
                    echo "- Application: http://${EXTERNAL_IP}:4173"
                    echo "- Admin Panel: http://${EXTERNAL_IP}:4174"
                    echo "- API: http://${EXTERNAL_IP}:4000"
                    echo "- Kibana Dashboard: http://${EXTERNAL_IP}:5601"
                    echo "- Elasticsearch: http://${EXTERNAL_IP}:9200"
                '''
            }
        }
    }

    
    post {
        success {
            echo '''
            ðŸŽ‰ Pipeline executed successfully!
            
            Your application and ELK Stack are now running:
            - Kibana Dashboard: Create index pattern "backend-logs-*" 
            - Monitor your Winston logs in real-time
            - Set up alerts and dashboards as needed
            '''
        }
        failure {
            sh '''
                echo "Pipeline failed. Collecting debug information..."
                echo "Docker Services Status:"
                docker compose ps
                
                echo "ELK Stack Logs:"
                docker compose logs elasticsearch | tail -50
                docker compose logs logstash | tail -50
                docker compose logs filebeat | tail -50
                
                echo "Application Logs:"
                docker compose logs backend | tail -50
            '''
            echo 'Pipeline failed. Please check the logs above for details.'
        }
        always {
            sh '''
                echo "Cleanup: Removing unused Docker resources..."
                docker system prune -f --volumes || true
            '''
        }
    }
}



services:
  frontend:
    build:
      context: ./prod-client
      dockerfile: Dockerfile
    ports:
      - "4173:4173"
    depends_on:
      - backend
    env_file:
      - ./prod-client/.env
    networks:
      - app-network

  admin:
    build:
      context: ./prod-admin
      dockerfile: Dockerfile
    ports:
      - "4174:4173"
    depends_on:
      - backend
    env_file:
      - ./prod-admin/.env
    networks:
      - app-network

  backend:
    build:
      context: ./prod-server
      dockerfile: Dockerfile
    ports:
      - "4000:4000"
    depends_on:
      mongodb:
        condition: service_healthy
      mongodb-init:
        condition: service_completed_successfully
      elasticsearch:
        condition: service_healthy
    env_file:
      - ./prod-server/.env.production
    environment:
      - NODE_ENV=production
    volumes:
      # Mount logs directory to make it accessible to Filebeat
      - ./prod-server/logs:/app/logs:rw
    networks:
      - app-network
    restart: unless-stopped

  mongodb:
    image: mongo:7.0
    container_name: mongodb
    restart: unless-stopped
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: password123
      MONGO_INITDB_DATABASE: prod-db
    volumes:
      - mongodb_data:/data/db
      - mongodb_config:/data/configdb
      - ./mongo-keyfile:/opt/keyfile:ro
    networks:
      - app-network
    command: >
      mongod --auth 
       --replSet rs0 
       --bind_ip_all 
       --keyFile /opt/keyfile/mongodb-keyfile
    healthcheck:
      test:
        [
          "CMD",
          "mongosh",
          "--port",
          "27017",
          "--quiet",
          "--eval",
          "db.adminCommand('ping')",
        ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 40s

  mongodb-init:
    image: mongo:7.0
    depends_on:
      mongodb:
        condition: service_healthy
    networks:
      - app-network
    command: >
      mongosh --host mongodb:27017 --username admin --password password123 --authenticationDatabase admin --eval "
        try {
          rs.status();
          print('Replica set already initialized');
        } catch (e) {
          if (e.message.includes('no replset config has been received') || e.message.includes('not running with --replSet')) {
            print('Initializing replica set...');
            rs.initiate({
              _id: 'rs0',
              members: [{ _id: 0, host: 'mongodb:27017' }]
            });
            print('Waiting for replica set to be ready...');
            while (rs.isMaster().ismaster !== true) {
              sleep(1000);
            }
            print('Replica set initialized successfully');
          } else {
            print('Error: ' + e.message);
            quit(1);
          }
        }
      "
    restart: "no"

  # ELK Stack Services
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m" # Reduced for GCP VM
      - bootstrap.memory_lock=true
      - cluster.routing.allocation.disk.threshold_enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      - app-network
    healthcheck:
      test:
        ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s # Increased for GCP VM

  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    container_name: logstash
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
      - ./prod-server/logs:/logs:ro
    ports:
      - "5044:5044"
      - "5000:5000/tcp"
      - "5000:5000/udp"
      - "9600:9600"
    environment:
      LS_JAVA_OPTS: "-Xmx256m -Xms256m" # Reduced for GCP VM
    networks:
      - app-network
    depends_on:
      elasticsearch:
        condition: service_healthy
    restart: unless-stopped

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    networks:
      - app-network
    depends_on:
      elasticsearch:
        condition: service_healthy

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.0
    container_name: filebeat
    user: root
    volumes:
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - ./prod-server/logs:/logs:ro
      - filebeat_data:/usr/share/filebeat/data
    environment:
      - output.elasticsearch.hosts=["elasticsearch:9200"]
    networks:
      - app-network
    depends_on:
      elasticsearch:
        condition: service_healthy
    command: filebeat -e -strict.perms=false

networks:
  app-network:
    driver: bridge

volumes:
  mongodb_data:
    driver: local
  mongodb_config:
    driver: local
  elasticsearch_data:
    driver: local
  filebeat_data:
    driver: local








# ELK Stack Setup for GCP VM with Winston JSON Logs

## Prerequisites for GCP VM

### 1. VM Requirements
Your GCP VM should have at least:
- **4GB RAM** (Elasticsearch needs memory)
- **20GB disk space**
- **Ubuntu 20.04 or later**

### 2. Update VM Memory Settings
```bash
# Check current memory
free -h

# If you have less than 4GB, consider upgrading your VM instance
# Or reduce Elasticsearch memory in docker-compose:
# ES_JAVA_OPTS: "-Xms512m -Xmx512m"
```

### 3. Install Docker and Docker Compose (if not already installed)
```bash
# Update packages
sudo apt update

# Install Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER

# Install Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/download/v2.20.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# Logout and login again to apply docker group changes
exit
# SSH back into your VM
```

## Step-by-Step Setup

### Step 1: Prepare Your Project Directory
```bash
cd /path/to/your/project
```

### Step 2: Create ELK Configuration
Run this script to create all necessary configuration files:

```bash
#!/bin/bash

# Create necessary directories
mkdir -p logstash/config
mkdir -p logstash/pipeline  
mkdir -p filebeat

# Create Filebeat configuration for Winston JSON logs
cat > filebeat/filebeat.yml << 'EOF'
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /logs/development.log
    - /logs/production.log
  fields:
    service: backend
  fields_under_root: true
  # Handle JSON multiline logs
  multiline.pattern: '^\{'
  multiline.negate: true
  multiline.match: after
  multiline.max_lines: 500
  exclude_lines: ['^$']
  # Parse JSON at filebeat level
  json.keys_under_root: true
  json.add_error_key: true
  
output.elasticsearch:
  hosts: ["elasticsearch:9200"]
  index: "backend-logs-%{+yyyy.MM.dd}"

setup.template:
  name: "backend-logs"
  pattern: "backend-logs-*"
  enabled: true

logging.level: info
EOF

# Create Logstash configuration
cat > logstash/config/logstash.yml << 'EOF'
http.host: "0.0.0.0"
xpack.monitoring.elasticsearch.hosts: [ "http://elasticsearch:9200" ]
path.config: /usr/share/logstash/pipeline
EOF

# Create Logstash pipeline for Winston logs
cat > logstash/pipeline/logstash.conf << 'EOF'
input {
  beats { port => 5044 }
}

filter {
  # Parse JSON logs from Winston
  if [message] {
    json { source => "message" }
  }
  
  # Convert Winston timestamp format to Elasticsearch
  if [timestamp] {
    date {
      match => [ "timestamp", "dd MMM yyyy HH:mm" ]
      target => "@timestamp"
      timezone => "Asia/Kolkata"
    }
  }
  
  # Determine environment from file path
  if [log][file][path] =~ /development\.log$/ {
    mutate {
      add_field => { "log_type" => "development" }
      add_field => { "environment" => "development" }
    }
  } else if [log][file][path] =~ /production\.log$/ {
    mutate {
      add_field => { "log_type" => "production" }  
      add_field => { "environment" => "production" }
    }
  }
  
  # Handle error objects in meta field
  if [meta] {
    ruby {
      code => "
        meta = event.get('meta')
        if meta.is_a?(Hash)
          meta.each do |key, value|
            if value.is_a?(Hash) && value['name'] && value['message'] && value['trace']
              event.set('error_name', value['name'])
              event.set('error_message', value['message']) 
              event.set('error_stack', value['trace'])
            end
          end
        end
      "
    }
  }
  
  mutate {
    add_field => { "application" => "backend" }
    add_field => { "service" => "backend" }
    remove_field => [ "host", "agent", "ecs", "input" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "backend-logs-%{+YYYY.MM.dd}"
  }
}
EOF

echo "Configuration files created!"
```

### Step 3: Update Docker Compose
Replace your docker-compose.yml with the version that includes ELK stack services.

### Step 4: Configure GCP Firewall (if accessing externally)
```bash
# If you want to access Kibana from outside the VM
gcloud compute firewall-rules create allow-kibana \
  --allow tcp:5601 \
  --source-ranges 0.0.0.0/0 \
  --description "Allow Kibana access"

# For Elasticsearch (optional, for debugging)
gcloud compute firewall-rules create allow-elasticsearch \
  --allow tcp:9200 \
  --source-ranges 0.0.0.0/0 \
  --description "Allow Elasticsearch access"
```

### Step 5: Increase VM Memory Limits
```bash
# Increase max map count for Elasticsearch
sudo sysctl -w vm.max_map_count=262144

# Make it permanent
echo 'vm.max_map_count=262144' | sudo tee -a /etc/sysctl.conf
```

### Step 6: Start the Stack
```bash
# Make sure your logs directory exists
ls -la prod-server/logs/

# Start all services
docker-compose up -d

# Check service status
docker-compose ps

# Monitor logs
docker-compose logs -f elasticsearch
docker-compose logs -f logstash  
docker-compose logs -f kibana
docker-compose logs -f filebeat
```

### Step 7: Verify Setup
```bash
# Check Elasticsearch health
curl http://localhost:9200/_cluster/health

# Check if indices are being created
curl http://localhost:9200/_cat/indices?v

# Check if logs are being indexed
curl http://localhost:9200/backend-logs-*/_search?pretty&size=5
```

### Step 8: Access Kibana
1. **Internal Access**: http://localhost:5601
2. **External Access**: http://YOUR_VM_EXTERNAL_IP:5601

### Step 9: Configure Kibana Index Pattern
1. Open Kibana in browser
2. Go to **Stack Management** â†’ **Index Patterns**
3. Create index pattern: `backend-logs-*`
4. Select `@timestamp` as time field
5. Go to **Discover** to view logs

## Your Winston Log Structure in Kibana

Your logs will appear in Kibana with this structure:
```json
{
  "@timestamp": "2025-08-05T14:30:00.000Z",
  "level": "INFO",
  "message": "Your log message",
  "timestamp": "05 Aug 2025 14:30",
  "meta": {
    // Your metadata
  },
  "service": "backend",
  "environment": "production",
  "log_type": "production"
}
```

## Useful Kibana Queries

- **Filter by log level**: `level: "ERROR"`
- **Filter by environment**: `environment: "production"`  
- **Search in message**: `message: "database"`
- **View errors with stack traces**: `error_name: *`

## Troubleshooting

### Common Issues:

1. **Elasticsearch won't start**:
   ```bash
   # Check memory
   free -h
   # Increase VM memory or reduce ES heap size
   ```

2. **No logs appearing**:
   ```bash
   # Check filebeat logs
   docker-compose logs filebeat
   
   # Check if log files exist
   docker-compose exec backend ls -la /app/logs/
   ```

3. **Permission issues**:
   ```bash
   # Fix log file permissions
   sudo chmod 644 prod-server/logs/*.log
   ```

4. **Memory issues**:
   ```bash
   # Monitor memory usage
   docker stats
   
   # Reduce memory allocation in docker-compose.yml
   ```

## Performance Optimization

For better performance on GCP VM:

1. **Use SSD persistent disk**
2. **Increase VM memory to 8GB+ for production**
3. **Set up log rotation**:
   ```javascript
   // In your Winston config, add log rotation
   new transports.File({
     filename: 'logs/production.log',
     maxsize: 100 * 1024 * 1024, // 100MB
     maxFiles: 5,
     tailable: true
   })
   ```

4. **Enable index lifecycle management in Elasticsearch**

Your setup is now ready to visualize your Winston JSON logs in Kibana!